---
title: LM-KBC
description: Knowledge Base Construction from Pre-trained Language Models
---

### Task Description

Pre-trained language models (LMs) have advanced a range of semantic tasks and have also shown promise for knowledge extraction from the models itself. Although several works have explored this ability in a setting called probing or prompting, the viability of _knowledge base construction_ from LMs has not yet been explored.
In this challenge, participants are asked to build actual knowledge bases from LMs, for given subjects and relations. In crucial difference to existing probing benchmarks like LAMA ([Petroni et al., 2019](https://arxiv.org/pdf/1909.01066.pdf)), we make no simplifying assumptions on relation cardinalities, i.e., a subject-entity can stand in relation with zero, one, or many object-entities. Furthermore, submissions need to go beyond just ranking the predictions, and materialize outputs, which are evaluated by established KB metrics of precision and recall. The challenge comes with two tracks: (i) a BERT-type LM track with low computational requirements, and (ii) an open track, where participants can use any LM of their choice.

### Dataset

### Evaluation Metrics

### Submission Details

### Important Dates

### People

- [Sneha Singhania](https://people.mpi-inf.mpg.de/~ssinghan/), Max Planck Institute for Informatics, Germany
- [Tuan-Phong Nguyen](https://www.tuan-phong.com/), Max Planck Institute for Informatics, Germany
- [Simon Razniewski](http://simonrazniewski.com/), Max Planck Institute for Informatics, Germany

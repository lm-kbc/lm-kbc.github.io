<html>

<head>
<meta http-equiv=Content-Type content="text/html; charset=windows-1252">
<meta name=Generator content="Microsoft Word 15 (filtered)">
<style>
<!--
 /* Font Definitions */
 @font-face
	{font-family:"Cambria Math";
	panose-1:2 4 5 3 5 4 6 3 2 4;}
@font-face
	{font-family:Calibri;
	panose-1:2 15 5 2 2 2 4 3 2 4;}
 /* Style Definitions */
 p.MsoNormal, li.MsoNormal, div.MsoNormal
	{margin-top:0in;
	margin-right:0in;
	margin-bottom:8.0pt;
	margin-left:0in;
	line-height:106%;
	font-size:11.0pt;
	font-family:"Calibri",sans-serif;}
h2
	{mso-style-link:"Heading 2 Char";
	margin-top:2.0pt;
	margin-right:0in;
	margin-bottom:0in;
	margin-left:0in;
	line-height:106%;
	page-break-after:avoid;
	font-size:16.0pt;
	font-family:"Arial",sans-serif;
	color:black;
	font-weight:normal;}
a:link, span.MsoHyperlink
	{color:blue;
	text-decoration:underline;}
p.MsoPlainText, li.MsoPlainText, div.MsoPlainText
	{mso-style-link:"Plain Text Char";
	margin:0in;
	font-size:11.0pt;
	font-family:"Calibri",sans-serif;}
span.Heading2Char
	{mso-style-name:"Heading 2 Char";
	mso-style-link:"Heading 2";
	font-family:"Arial",sans-serif;
	color:black;}
span.PlainTextChar
	{mso-style-name:"Plain Text Char";
	mso-style-link:"Plain Text";
	font-family:"Calibri",sans-serif;}
.MsoChpDefault
	{font-size:10.0pt;
	font-family:"Calibri",sans-serif;}
.MsoPapDefault
	{margin-bottom:8.0pt;
	line-height:106%;}
@page WordSection1
	{size:595.3pt 841.9pt;
	margin:1.0in 1.0in 1.0in 1.0in;}
div.WordSection1
	{page:WordSection1;}
-->
</style>

</head>

<body lang=EN-US link=blue vlink="#954F72" style='word-wrap:break-word'>

<div class=WordSection1>

<p class=MsoNormal style='line-height:normal'><b><span style='font-size:24.0pt;
font-family:"Arial",sans-serif'>Knowledge Base Construction from Pre-trained
Language Models (LM-KBC) </span></b></p>

<p class=MsoNormal><span style='font-family:"Arial",sans-serif'><a
href="https://iswc2023.semanticweb.org/" target="_blank">Workshop @ 22<sup>nd</sup>
International Semantic Web Conference (ISWC 2023) </a></span></p>

<p class=MsoNormal><span style='font-family:"Arial",sans-serif'>&nbsp;</span></p>

<h2>Overview</h2>

<p class=MsoPlainText><span style='font-family:"Arial",sans-serif'>Language
models (LMs), such as chatGPT, BERT, and T5, have demonstrated remarkable
outcomes in numerous AI applications. Research has shown that these models
implicitly capture vast amounts of factual knowledge within their parameters,
resulting in a remarkable performance in knowledge-intensive applications. The
seminal paper ``Language Models as Knowledge Bases?'' sparked interest in the
spectrum between language models and knowledge graphs, leading to a diverse
range of research on the usage of LMs for knowledge base construction. This
research includes:</span></p>

<p class=MsoPlainText><span style='font-family:"Arial",sans-serif'>(1)
utilizing pre-trained LMs for knowledge base completion and construction tasks,</span></p>

<p class=MsoPlainText><span style='font-family:"Arial",sans-serif'>(2)
performing information extraction tasks, like entity linking and relation
extraction, and</span></p>

<p class=MsoPlainText><span style='font-family:"Arial",sans-serif'>(3)
utilizing knowledge graphs to support LM applications.</span></p>

<p class=MsoPlainText><span style='font-family:"Arial",sans-serif'>In the first
edition of the LM-KBC workshop, we aim to give space to the emerging academic
community that investigates these topics, host extended discussions around the
LM-KBC Semantic Web challenge, and enable an informal exchange of researchers
and practitioners.</span></p>

<h2>Topics</h2>

<p class=MsoNormal><span style='font-family:"Arial",sans-serif'>- <span
style='transform: scaleX(1.04193)' role=presentation>Entity recognition and
disambiguation with LMs</span><br role=presentation>
<span role=presentation>- </span><span style='transform: scaleX(1.05436)' role=presentation>Relation
extraction with LMs</span><br role=presentation>
<span role=presentation>- </span><span style='transform: scaleX(1.00473)' role=presentation>Zero-shot
and few-shot knowledge extraction from LMs</span><br role=presentation>
<span role=presentation>- </span><span style='transform: scaleX(0.989405)' role=presentation>Consistency
of LMs</span><br role=presentation>
<span role=presentation>- </span><span style='transform: scaleX(1.01709)' role=presentation>Knowledge
consolidation with LMs</span><br role=presentation>
<span role=presentation>- </span><span style='transform: scaleX(1.00003)' role=presentation>Comparisons
of LMs for KBC tasks</span><br role=presentation>
<span role=presentation>- </span><span style='transform: scaleX(1.01323)' role=presentation>Methodological
contributions on training and fine-tuning LMs for KBC tasks</span><br
role=presentation>
<span role=presentation>- </span><span style='transform: scaleX(1.01081)' role=presentation>Evaluations
of downstream capabilities of LM-based KGs in tasks like QA</span><br
role=presentation>
<span role=presentation>- </span><span style='transform: scaleX(1.00034)' role=presentation>Designing
robust prompts for large language model probin</span>g</span></p>

<h2>Tentative schedule</h2>

<p class=MsoNormal style='margin-bottom:0in;line-height:normal'><span
style='font-family:"Arial",sans-serif'>The idea of this workshop is to provide
a focused venue for informal exchange around LMs for KB construction. In
particular, we plan to devote space to extended presentations of participants
of the LM-KBC ISWC challenge (where the 1-hour ISWC conference slot will only
allow presentations of the winners). Furthermore, we plan to make this a hybrid
event to lower the entry barrier for participants.</span></p>

<p class=MsoNormal style='margin-bottom:0in;line-height:normal'><span
style='font-size:12.0pt;font-family:"Arial",sans-serif'><br>
</span><span style='font-family:"Arial",sans-serif'>- 10:00-10:15: Welcome</span><span
style='font-size:12.0pt;font-family:"Arial",sans-serif'><br>
</span><span style='font-family:"Arial",sans-serif'>- 10:15-11:00: Keynote 1</span><span
style='font-size:12.0pt;font-family:"Arial",sans-serif'><br>
</span><span style='font-family:"Arial",sans-serif'>- 11:00-11:30: Coffee Break</span><span
style='font-size:12.0pt;font-family:"Arial",sans-serif'><br>
</span><span style='font-family:"Arial",sans-serif'>- 11:30-12:30: Paper
Presentations</span><span style='font-size:12.0pt;font-family:"Arial",sans-serif'><br>
</span><span style='font-family:"Arial",sans-serif'>- 12:30-13:30: Lunch Break</span><span
style='font-size:12.0pt;font-family:"Arial",sans-serif'><br>
</span><span style='font-family:"Arial",sans-serif'>- 13:30-14:00: Keynote 2
(perhaps hybrid)</span><span style='font-size:12.0pt;font-family:"Arial",sans-serif'><br>
</span><span style='font-family:"Arial",sans-serif'>- 14:00-15:00: LM-KBC
Challenge Presentations</span><span style='font-size:12.0pt;font-family:"Arial",sans-serif'><br>
</span><span style='font-family:"Arial",sans-serif'>- 15:00-15:20: Coffee Break</span><span
style='font-size:12.0pt;font-family:"Arial",sans-serif'><br>
</span><span style='font-family:"Arial",sans-serif'>- 15:20-16:00: Paper
Presentations</span><span style='font-size:12.0pt;font-family:"Arial",sans-serif'><br>
</span><span style='font-family:"Arial",sans-serif'>- 16:00-16:15: Closing
Ceremony</span></p>

<p class=MsoNormal style='margin-bottom:0in;line-height:normal'><span
style='font-family:"Arial",sans-serif'>&nbsp;</span></p>

<h2>Review process</h2>

<p class=MsoNormal style='margin-bottom:0in;line-height:normal'><span
style='font-family:"Arial",sans-serif'>Papers will be peer-reviewed by at least
three researchers using a double-blind review. Selected papers will be
published on CEUR (if the authors agree to have their papers published). We
invite the following types of papers:<br role=presentation>
<span role=presentation>- </span><span style='transform: scaleX(0.972496)' role=presentation>Full
research papers: novel research contributions (8-12 pages)</span><br
role=presentation>
<span role=presentation>- </span><span style='transform: scaleX(0.964882)' role=presentation>Short
research papers: novel research contributions of smaller scope than full</span>
papers (3-5 pages)<br role=presentation>
<span role=presentation>- </span><span style='transform: scaleX(1.05738)' role=presentation>Position
papers: presenting a novel idea that is not yet in the scope of a</span>
research contribution (6-8 pages)<br role=presentation>
<span role=presentation>- </span><span style='transform: scaleX(0.952488)' role=presentation>Demo
papers: presenting a system based on research concepts (6-8 pages)</span></span></p>

<p class=MsoNormal style='margin-bottom:0in;line-height:normal'><span
style='font-family:"Arial",sans-serif'><br>
<span style='transform: scaleX(0.99093)' role=presentation>Robert Bosch GmbH
has signaled that they would likely sponsor a best</span> paper award over 500
Euro.</span></span></p>

<p class=MsoNormal style='margin-bottom:0in;line-height:normal'><span
style='font-family:"Arial",sans-serif'>&nbsp;</span></p>

<h2>Chairs</h2>

<p class=MsoNormal style='margin-bottom:0in;line-height:normal'><b><span
style='font-family:"Arial",sans-serif'>Jan-Christoph Kalo</span></b><span
style='font-family:"Arial",sans-serif'><span role=presentation> </span><span
style='transform: scaleX(0.975008)' role=presentation>is a postdoctoral
researcher at the Learning and Reasoning Group at the Vrije Universiteit
Amsterdam. His research focus is on machine</span> learning methods,
particularly language models, for knowledge base construction and knowledge
management. He is co-organizing this year’s LM-KBC challenge at ISWC and has
been part of the PC of ISWC and ESWC.<br role=presentation>
<b><span style='transform: scaleX(1.10947)' role=presentation>Sneha Singhania</span></b><span
role=presentation> </span><span style='transform: scaleX(1.06489)' role=presentation>is
a PhD student at the Max-Planck Institute for Informatics. Her research focuses
on enabling machines to select and utilize high-</span>quality information
sources, with an awareness of unknowns, for knowledge base (KB) construction.
She co-organized the LM-KBC’22 challenge and has held PC roles at ACL, CIKM,
ISWC, and ESWC.<br role=presentation>
<b><span style='transform: scaleX(1.11065)' role=presentation>Simon Razniewski</span></b><span
role=presentation> </span><span style='transform: scaleX(0.997191)' role=presentation>is
a research scientist in the NLP and Neuro-Symbolic</span> AI group at the Bosch
Center for AI. He has previously organized the Wikidata workshop @ ISWC, and
the LM-KBC challenge 2022. He has also held senior roles in program committees
of major conferences such as IJCAI’21 and EACL’23 (area chair), or ISWC’20 and
CIKM’20 (senior PC member).<br role=presentation>
<b><span style='transform: scaleX(1.19841)' role=presentation>Jeff Z. Pan</span></b><span
role=presentation> </span><span style='transform: scaleX(1.05571)' role=presentation>is
a chair of the Knowledge Graph Group at the Alan Turing</span> Institute and is
a member of the School of Informatics at The University of Edinburgh. He is an
Editor of the Journal of Web Semantics (JoWS) and a Programme Chair of the 19th
International Semantic Web Conference (ISWC 2020), the premier international
forum for the Semantic Web / Knowledge Graph / Linked Data communities.</span></p>

<p class=MsoNormal style='margin-bottom:0in;line-height:normal'><span
style='font-family:"Arial",sans-serif'>&nbsp;</span></p>

<h2>References</h2>

<p class=MsoNormal style='margin-bottom:0in;line-height:normal'><span
style='font-family:"Arial",sans-serif'>- Petroni, F., Rocktaeschel, T., Riedel,
S., Lewis, P., Bakhtin, A., Wu, Y., Miller, A.: Language models as knowledge bases?
EMNLP, 2019</span></p>

<p class=MsoNormal style='margin-bottom:0in;line-height:normal'><span
style='font-family:"Arial",sans-serif'>- Blerta Veselhi, Sneha Singhania, Simon
Razniewski, Gerhard Weikum. Evaluating Language Models for Knowledge Base
Completion, ESWC, 2023</span></p>

<p class=MsoNormal style='margin-bottom:0in;line-height:normal'><span
style='font-family:"Arial",sans-serif'>- LM-KBC: Knowledge Base Construction
from Pre-trained Language Models, Sneha Singhania, Tuan-Phong Nguyen, Simon
Razniewski, CEUR-WS, 2022</span></p>

<p class=MsoNormal style='margin-bottom:0in;line-height:normal'><span
style='font-family:"Arial",sans-serif'>- Alivanistos, Dimitrios, Selene Báez
Santamaría, Michael Cochez, Jan-Christoph Kalo, Emile van Krieken, and Thiviyan
Thanapalasingam. &quot;Prompting as Probing: Using Language Models for Knowledge
Base Construction.&quot; LM-KBC, 2022.</span></p>

<p class=MsoNormal style='margin-bottom:0in;line-height:normal'><span
style='font-family:"Arial",sans-serif'>- Simon Razniewski, Andrew Yates, Nora
Kassner, Gerhard Weikum. Language Models As or For Knowledge Bases, DL4KG@ISWC,
2021</span></p>

<p class=MsoNormal style='margin-bottom:0in;line-height:normal'><span
style='font-family:"Arial",sans-serif'>- KAMEL: Knowledge Analysis with
Multitoken Entities in Language Models, JC Kalo, L Fichtel, AKBC 2022</span></p>

<p class=MsoNormal style='margin-bottom:0in;line-height:normal'><span
style='font-family:"Arial",sans-serif'>- Cohen, Roi, et al. &quot;Crawling The Internal
Knowledge-Base of Language Models.&quot; Findings of EACL 2023.</span></p>

<p class=MsoNormal style='margin-bottom:0in;line-height:normal'><span
style='font-family:"Arial",sans-serif'>- How much knowledge can you pack into
the parameters of a language model? Roberts, Adam, Colin Raffel, and Noam
Shazeer. &quot;How Much Knowledge Can You Pack Into the Parameters of a
Language Model?.&quot; EMNLP 2020.</span></p>

</div>

</body>

</html>
